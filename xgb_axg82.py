# -*- coding: utf-8 -*-
"""XGB 0.9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CZ4kp92rAEN8YMEq6DDX73oXf6YAocm_
"""

# Built by : Aniket Gupta using google colab

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import os
import xgboost as xgb
import operator
from matplotlib import pylab as plt
from sklearn import preprocessing

#Loading the data 
train = pd.read_csv("/content/drive/My Drive/Colab Notebooks/train_data.csv")
test = pd.read_csv("/content/drive/My Drive/Colab Notebooks/test_data.csv")
sample = pd.read_csv('/content/drive/My Drive/Colab Notebooks/samplesubmission.csv')

# Spliting the data and droping column headers 

labels = train.category.values
labels = preprocessing.LabelEncoder().fit_transform(labels)
train = train.drop(["key", "category"], axis=1)
features=list(train.columns[0:])
test = test.drop("key", axis = 1)

# xGBoosT model being used, where the evaluation metric is log loss

params = {"objective": "multi:softprob", "eval_metric":"mlogloss", "num_class": 9}
train_xgb = xgb.DMatrix(train, labels)
test_xgb  = xgb.DMatrix(test)

# The model was trained for several rounds, but it was surprising that the best performance came after the just 1 round of training, as the training rounds increased the model's accuracy decreased.

trainRound = 1
gbm = xgb.train(params, train_xgb, trainRound)  # training the model
pred = gbm.predict(test_xgb)                    # predicting using the test data

# Creating output CSV
pred = pd.DataFrame(pred, index=sample.key.values, columns=sample.columns[1:])
pred.to_csv('Final.csv', index_label='key')

# Downloading output CSV from google colab
from google.colab import files
files.download('Final.csv')
